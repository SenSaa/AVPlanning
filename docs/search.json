["Autonomous Vehicle Planning * This is an overview of planning and decision-making in the field of autonomous vehicles, where we will introduce general planning algorithms and methods, and then shine some light on the techniques used in industry. For a change, it’s time to focus on the real world where academia publications are implemented and put to the test road! Autonomous vehicles components: Modular architecture/stack: Sensors/Cameras -> Localisation & Mapping + Perception -> Prediction -> Planning -> Control -> Actuation Output (Steer, Accelerate, Brake) Main blocks: Perception (Sensors/Cameras Computer Vision/Localisation & Mapping) -> Planning/Decision Making (behaviour Prediction/Path planning/Control) Approaches, methodologies, and techniques used in the different autonomy blocks: Perception: Computer Vision, Neural Networks, Deep Learning -> CNNs, YOLO, R-CNN. Localisation & Mapping: Filters (Kalman, Patricle), SLAM. Behaviour Prediction: Supervised Deep learning -> RNN, LSTM, Unsupervised -> Clustering, Transformers. Path Planning: Search Algorithms (Dijkstra, A* variants), Sampling-based (RRT variants), Optimisation-based (geometric curve interpolation), or Reinforcement Learning (Value- based, Policy-based, Actor-Critic, AlphaGo and its variants), Imitation Learning (Behaviour Cloning, IRL, GAN). Control: (Speed) -> PID, (Steer) -> Pure-Pursuit, MPC, LQR. Perception Planning Perception Planning Localisation & Mapping Control Prediction S e n s o r s A c t u a t i o n ","A look at DARPA urban challenge 2007 – where self driving transformed from dream into reality: Planning techniques used in DARPA urban grand challenge: CMU Stanford MIT Global Planning (Structured) Graph search Dynamic Programming A* Behaviour Planning (Structured) Finite State Machine (FSM). Finite State Machine (FSM). Finite State Machine (FSM). Local Planning (Structured) Model Predictive Trajectory Generator on State Lattice. Model Predictive Trajectory Generator on a State Lattice. Closed Loop RRT (Biased-sampling RRT + vehicle model + Reeds Shepp). Unstructured Planning Anytime D* on State Lattice. Hybrid A* (A* + vehicle model + Reeds-Shepp + Conjugate Gradient + Voronoi Field). Closed Loop RRT (Biased-sampling RRT + vehicle model + Reeds Shepp). Control (Path Tracking) For curvature control, the control loop is primarily feed-forward control, supplemented by integral feedback for steady-state error correction. The speed control structure includes two non-linear PID controllers one for throttle and one for brake, with logic that switches between the two, depending on the speed error. PI Longitudinal (Speed) controller + Pure Pursuit for Lateral (Steering) controller. Interesting notes about the approaches used are: - Geometric/Graph search techniques were common for route (global) planning, with Stanford’s approach being one of the exceptions by using dynamic programming. - CMU and stanford’s trajectory (local) planning approaches were similar. - MIT used the same planning approach for both structured local planning and unstructured planning. ","Planning Algorithms methods: Global Planning: Graph Search:- Global Planning in Structured Environments: A baseline algorithm for searching through a graph is Dijkstra, it is useable for global planning in a structured environment such as on-road route planning for autonomous cars. However, in environment with denser connections of edges and nodes, A* would perform better as it’s guided by heuristics to the goal. In cases, where the A* algorithm is not fast enough, there are ways and variants to offer more speed, an example of that would be weighted A*, which applies a weight to the cost function, to allow a greedy search to the goal. Another option is to search in both directions, from the start forward and from the goal backwards, and from observations, bi-directional A* search would outperform a one-directional A*, since if searching in one direction gets stuck in a local minima, the search in the other direction can traverse towards it. And secondly, two small trees are probably more efficient than one big one. Another interesting approach to speed-up A* search is a pruning technique, JPS, where path symmetries are exploited and nodes are skipped (jump). This technique works on uniform-cost graphs. Finally, there are also parallel variants that enable search on multi-threads. Global Planning in Un-structured Environments: A variant of A* that has been extended to plan for the non-holonomic constraints of autonomous vehicles is the Hybrid-A*. Hybrid-A* is essentially A*, embedded with a vehicle (kinematic bicycle) model, Reeds-Shepp for the shortest distance between two cars that can reverse without obstacles, Conjugate Gradient optimisation for smoothing, as well as Voronoi field to speed-up collision detection. <- Used by Stanford DARPA Urban challenge entry. For dealing with Dynamic/Uncertain environments, incremental search techniques are also suitable. Examples of incremental graph search would be Dynamic A* Lite (D*-Lite) and Anytime D*. <- Used by CMU DARPA Urban challenge entry. Where a multi-resolution lattice was used, and a combined heuristic of environment constraints heuristic (standard heuristic 2D heuristic e.g. Euclidean, Manhattan, etc. that avoids occupancy/collisions) and a dynamics constraints heuristic (taking into account non-holonomic constraint -> cost of optimal solution through the search space computed offline and stored as a heuristic lookup table, Reeds Shepp can be an option here too). ","State/Search space discretisation: When using a graph-search technique, there are multiple ways to discretise the state/search space. In unstructured global planning, grids are usually used, it was the case for Hybrid-A* algorithm, but also state-lattice is a good option, which was demonstrated by CMU in the DARPA urban challenge, when they used solved the parking problem by searching a state-lattice with Anytime D*. For structured global planning directed weighted graph is used, such as adjacency list. Sampling-based:- A starting point for sampling-based planners is Rapidly-exploring Random Tree, RRT samples random states within the state space to connect an initial state to a goal state. Sampling-based techniques show their strength compared to graph-search techniques as the dimensions increase. Vanilla RRT is not optimal, which is where the asymptotically optimal RRT* comes in, it extends the base RRT algorithm with the ability to rewire the tree as well as best neighbour search. To speed-up RRTs, one way is to bias the search probability towards the goal. The most straightforward way is to goal bias the search with a goal bias probability threshold to improve the search efficiency. A more advanced approach is to employ a heuristic to focus the search, which leads to variants such Informed RRT*. Another direction is to use two trees, one from the start to goal as usual, and one from the goal back to start, and to connect the trees as they meet. This is the RRT-Connect algorithm which is one of the fastest, if not the fastest RRT variant to date. A flavour of RRT that was used in the DARPA urban challenge by MIT used an version of the RRT algorithm with Reeds Shepp paths for the steering/extension of the connections between the nodes planning over closed-loop dynamics and with biased sampling. The global planner used was A*, which sets the goal position (in local frame) to the next RNDF waypoint ~50m ahead. The global planner would run at 2Hz, while the motion planner was running at 10Hz. Path smoothing and optimisation: Another important sub-step during global planning or local planning, whether a graph search or a sampling based technique is used, is to smoothing/optimising the path. In many cases, these planning techniques produce straight line paths with sharp connections that are not suitable for non-holonomic platforms, therefore, there has to be a post-process step to generate a smooth continuous path. Strategies for smoothing/optimising paths can be divided into three categories: o Interpolation-based (Polynomial interpolation, Bezier Curve, Spline, B-Spline). o Take into consideration the vehicle model - non-holonomic constraint (Dubins, Reeds Shepp). o Optimisation-based (Gradient Descent/Ascent). ","Behaviour Planning: FSM: ▪ A classical approach to motion planning is to use a FSM to define the rules and states, and as the rules of driving are finite and limited, the majority of behaviours can covered, the long tail/edge cases will pose a problem however, without the ability to learn (learning ~ adapting). Imitation Learning: ▪ Instead of using perception and planning/decision making modules, we can go end to end with deep learning by taking the sensor inputs (cameras mainly) -> deep learning model (e.g. CNNs) -> Actuation outputs (Steering, Acceleration, Brake). Reinforcement Learning: ▪ Although reinforcement learning can be used for the path planning module (while handling perception separately), it can also used as an end to end method, the same way as with IL. Local Planning: Trajectory planning approaches can be generally divided into two main categories: ▪ Search based – Exploration by building a tree either deterministically with graph search or through random sampling. ▪ Optimisation based – Optimise (maximise/minimise) a cost objective function. Optimisation based methods iteratively refine a solution until termination/convergence conditions are met. The state space is discretised for search-based planning, while it’s continuous for optimisation-based planning. Search-based Optimisation-based State space Discretised Continuous Global Optimum + Global Optimum - Local Optima Smoothness/Continuity - Discontinuous + Smooth/Continuous Convergence speed - State Space dependent (Curse of Dimensionality) + Fast Convergence Search-based methods guarantees global optimum, while optimisation-based methods can fall into local minima for non-convex problems which is the case for planning. Optimisation-based approaches produce smooth trajectories due the continuous state space. They can also converge quickly towards the gradient direction. ","Search based techniques then can be categorised as: • Search based – Deterministic search of discretised environment (e.g. A*). Explore/traverse a discretised state space (graph) according to some strategy (heuristic). • Sampling based – Stochastic exploration of environment (e.g. RRT). Search non-convex high dimensional state spaces by randomly building a tree. There are two main steps in search-based approaches: ➢ Trajectory Generation ➢ Graph Search Optimisation based methods fall into: • Interpolating curve planners – Optimisation of the curvature and smoothness (comfort) of the path. It includes lines/circles, polynomials, splines, beziers, and clothoids. • Numerical optimisation – Optimise (maximise/minimise) a function in terms of constraints. Function optimisation Search Optimisation Search Sampling Interpolation Curve Numerical Optimisation Dijkstra A* D* Lite AD* DP RRT RRT* PRM Lines & Circles (RS) Polynomial Spline Bezier Clothoid QP, iLQR, MPC Trajectories can be generated in the state space or in the control space. Forward kinematics - the initial state of the robot and control input are given, and the final state can be obtained through integrating the control input. Typical numerical integration methods include the Euler, Simpson, and Runge-Kutta methods. Reverse kinematics - In inverse kinematics, the initial and final state of the robot are given, and the control input is to be determined. It usually requires solving a system of algebraic equations if the trajectory can be represented by polynomials or a trajectory optimization problem for general nonlinear systems. ","Vehicle Model: A vehicle model is used to satisfy the non-holonomic constraints of car-like vehicles. A kinematic or dynamic vehicle model can be used. There are numerous ways to model a vehicle including bicycle model, Dubins car, car pulling trailer. A common approach is the kinematic bicycle model. ̇ =    ̇ =    ̇ =   /  ̇ = Where: (x,y) - global coordinates of the rear wheel. Θ - orientation of the vehicle in the global frame (can also be called yaw or heading of vehicle). δ - steering angle in the body frame. ν – longitudinal velocity along the body at the rear wheel. ω – angular velocity of the steering wheel. L - distance between the wheels (wheel-base). v L δ θ y θ (x,y) x ","Search-based Local/Trajectory motion Planning: Here is a list of ways on how graph search can be used for vehicle motion planning: - For structured on-road planning, configure the graph search space locally. - Discretising the search space graph into a state lattice, so that the motions from the start state to the goal can be feasible for a vehicle. - Search a grid state space, and then smoothing the final path using a curve interpolation path optimisation step. - For fast path finding in a uniform grid graph, jump point search can be used which is a fast variant of A*. Sampling-based Local/Trajectory motion Planning: Sampling-based planners such as RRT (which is what will be mainly discussed here), can be used for local/trajectory motion planning. There are a number of ways to extend RRT and it’s variants to be suitable for kino-dynamic and more specifically vehicle-based motion planning. Here is a list of ways to enable RRT to be suitable for vehicle motion planning: - After generating a path using the RRT algorithm, smooth path through curve interpolation (spline, Bezier, etc.). - Incorporate vehicle model into the steering/extend function of the RRT algorithm. - Incorporate a controller instead of just a vehicle model for the steering/extend function of the RRT algorithm, computing control inputs through forward simulations. - Use Dubins/Reeds Shepp curve to for the steer/extend function instead of the standard straight line connection between the sampled node and the nearest node. - Utilise a clothoid to compute the steering/extension procedure between the sampled node and the nearest node in the tree. - Combination of the above, using a vehicle model and/or curve interpolation, and/or another path optimisation after generating the final path. ML vs Classical Motion Planning: The state of the are in machine learning moves too fast. Initially, ML/DL was exclusively used in perception (object detection, classification, etc.) However, more recently, more ML is being deployed in planning, particularly in behaviour planning, as well as in unstructured planning and more. We will delve into that in the industry approaches section! ","Cases: Case 1: ChauffeurNet - Learning to drive by imitating (Waymo – Dec 2018). ChauffeurNet (Imitation learning approach) is a model that uses a CNN, and 2x RNN. Taking camera images as inputs to imitate the expert behaviour from the closed-loop training (avoid collisions and staying on desired trajectories). This model was not found to be competitive with existing motion planning techniques by Waymo. Discussion results: “In this paper, we presented our experience with what it took to get imitation learning to perform well in real-world driving. We found that key to its success is synthesizing interesting situations around the expert’s behavior and augmenting appropriate losses that discourage undesirable behavior. This constrained exploration is what allowed us to avoid collisions and off-road driving even though such examples were not explicitly present in the expert’s demonstrations. To support it, and to best leverage the expert data, we used middle-level input and output representations which allow easy mixing of real and simulated data and alleviate the burdens of learning perception and control. With these ingredients, we got a model good enough to drive a real car. That said, the model is not yet fully competitive with motion planning approaches but we feel that this is a good step forward for machine learned driving models. There is room for improvement: comparing to end-to-end approaches, and investigating alternatives to imitation dropout are among them. But most importantly, we believe that augmenting the expert demonstrations with a thorough exploration of rare and difficult scenarios in simulation, perhaps within a reinforcement learning framework, will be the key to improving the performance of these models especially for highly interactive scenarios.” Case 2: - Solving Car Parking problem (Tesla AI Day – 2021). In solving the car parking problem, A* was outperformed by RL by orders of magnitude, particularly with regards to nodes expansion in the search space, as shown in the table below. ","Joint/Multi-Agent Planning: Case 1: Multi-Agent AutoPilot (Tesla AI Day – 2021). Tesla does not run the AutoPilot planner only on the ego vehicle, but also on other agents in the scene. That way, optimal/realistic prediction rollouts can be retrieved. Sim 2 Real Gap – Transferring what was learned in simulation to the real-world: There are a number of approaches to tackle this challenge, including: • System identification o System identification is to build a mathematical model for a physical system; in the context of RL, the mathematical model is the simulator. To make the simulator more realistic, careful calibration is necessary. o Unfortunately, calibration is expensive. Furthermore, many physical parameters of the same machine might vary significantly due to temperature, humidity, positioning or its wear-and-tear in time. • Domain adaptation o Domain adaptation (DA) refers to a set of transfer learning techniques developed to update the data distribution in sim to match the real one through a mapping or regularization enforced by the task model. o Many DA models, especially for image classification or end-to-end image-based RL task, are built on adversarial loss or GAN. ","• Domain randomization o With domain randomization (DR), we are able to create a variety of simulated environments with randomized properties and train a model that works across all of them. o Domain randomisation can be static, adaptive or adversarial (disturbances/ perturbations). For example, Observation/parameter Noise. o Likely this model can adapt to the real-world environment, as the real system is expected to be one sample in that rich distribution of training variations. Also, - Learning to adapt the textures of the simulator to match the real domain - Learning higher level policies, not low-level controllers, as the low level dynamics are very different between Sim and Real. The randomization parameters can control appearances of the scene, including but not limited to: • Position, shape, and colour of objects, • Material texture, • Lighting condition, • Random noise added to images, • Position, orientation, and field of view of the camera in the simulator. Physical dynamics in the simulator can also be randomized. Studies have shown that a recurrent policy can adapt to different physical dynamics including the partially observable reality. A set of physical dynamics features include but are not limited to: • Mass and dimensions of objects, • Mass and dimensions of robot bodies, • Damping, kp, friction of the joints, • Gains for the PID controller (P term), • Joint limit, • Action delay, • Observation noise. With visual and dynamics DR, OpenAI Robotics were able to learn a policy that works on real dexterous robot hand. ","Industry Approaches: Summary of trends in the industry: Cruise: Cruise Decision Making Architecture ✓ ML trajectory seeds [Prediction] - Understanding uncertainty (kinematic and *existence). * Existence uncertainty is when the actor is not seen/known e.g. occlusion. ✓ Imitation Learning (IL) [Planning] - Resolve interactions/conflicts. ✓ Reinforcement Learning (RL) [Planning] - Understanding uncertainty from modelling errors. ✓ Motion planning, optimal control, robust control [Mostly Control, could also include planning] - Incorporating models of the vehicle and the road into the decision to drive safely and comfortably. In terms of techniques used: - To address conflict/interaction resolution, one of the approaches (possibly the main one) is by auto labelling ~100k examples to understand the resolution of discrete decisions such as overtaking via imitation learning offline, and then the models are used online in the costing system to select the right action. - Reinforcement learning is used to train a policy offline to understand scenarios that is not according to our predictions or the situation is evolving different to expectation as the vehicle starts to drive (modelling errors), these are edge cases such as when pedestrians are close to the AV behaving in unpredictable ways. ","Reinforcement learning is also used to train the agent offline to learn complex interactions such as parking the vehicle with a simple goal of reaching destination (achieve goal pose), drive safely (avoid collision), behave comfortably (satisfactory acceleration and jerk I presume), and to consider vehicle dynamics. High-fidelity models are used to model the world and dynamics of the vehicle to achieve smooth and stable. Using techniques from motion planning, optimal control, robust control. Tesla: 1 st showcase - 2019: Imitation learning is used for path prediction and decision making (planning). Tesla driver’s are essentially annotating the data while driving, through acceleration and steering inputs, data is being collected that is supervising the system how traverse environments. Inputs: Video of the cameras, path from the GPS and IMU, wheel angle. This data is then used for supervision for the network, neural networks are trained on these trajectories taken, and then the NN can predict paths from all the collected fleet data. This is Imitation learning, trajectories from human manual drives are taken from the real world, and then the system will imitate it. 2 nd showcase – Unstructured and structured planning - 2021: Unstructured planning: In complex unstructured scenes, learning-based methods can solve the planning problem more efficiently. The learning-based state of the art approach (RL -> MuZero) was found to be orders of magnitude more efficient in terms of state expansions than standard conventional path planning (A* with Euclidean distance heuristic over a state lattice search space). It is challenging to design a global optimal heuristic. However, to find a global value function, neural networks can be employed. Tesla’s Perception/Vision networks produce a vector space, which can be treated as a game (like an Atari game), so Deep Mind’s approaches can be used to solve this class of problems like Alpha Zero and MuZero, etc. What’s taking place here is, neural networks that create state and action distributions, that is then passed to a Monte Carlo Tree Search (MCTS) with various cost functions (same costs as noted earlier, in addition to driver interventions). ","Structured planning: The solution Tesla came up with is to break down the problem hierarchically, by first using a coarse search method, to crunch down the non-convexity and to come up with a convex corridor, and then use a convex function optimisation technique to create the final trajectory (for fast convergence). From the convex corridor created from the search, the spline, heading and acceleration are initialised, parameterised over the arc length of the plan, then the function optimisation continuously makes improvements to reduce its costs. These costs are distance to obstacles, traversal time, comfort (lateral acceleration and lateral jerk). When driving among other agents, planning is done jointly, to do this, AutoPilot is run on every single relevant object in the scene. The final architecture: Tesla High-Level Architecture - Vision system crushes down the dense video data into a vector space. - The vector space will be consumed by an explicit planner (search + optimisation) and Neural Net planner (RL + IL). - The neural net planner can also consume intermediate features in addition to the vector space. - Together, the vector space and intermediate features networks create a trajectory distribution, and this can be optimised end-to-end with explicit cost functions and human intervention, as well as imitation learning data. ","- This then goes into an explicit planning function that performs -> search + optimisation (as discussed earlier) and finally generates control commands. 3 rd showcase – More about structured planning - 2022: To resolve interactions, using multi-agent joint planning approach, interaction search is performed. This interaction search is a parallelised tree search over manoeuvre trajectories. The way this works is: ➢ Vision measurements (lanes, occupancy, moving objects). ➢ Goal candidates (Imitation Learning). ➢ Seed trajectories (classical optimisation + network planner that was trained from the customer fleet [Imitation Learning]). ➢ Branch the interactions, finding the best interaction (e.g. whether to assert or yield to a pedestrian). ➢ Building over this optimisation problem incrementally, with more and more constraints. Tesla Interaction Search Approach ","Trajectory Generation: Inside each node of the search tree, trajectories are created using: ▪ Classical optimisation approaches, where the constraints are added incrementally. ▪ Light-weight neural network that can run in the loop of the planner, these networks are trained on human demonstrations (Imitation Learning) from the fleet + offline solvers with relaxed time limits. Trajectory Scoring: - Trajectories are scored based on the following criteria: ✓ Collision Checks. ✓ Comfort analysis (lateral acceleration and jerk). ✓ Intervention likelihood – Neural network train with interventions data from the fleet. It gives a score of how likely a given manoeuvre is to result in an intervention over the next few seconds. ✓ Human-like Discriminator – Imitation learning from human driven fleet data neural network, with a score of how close the given action is to a human driven trajectory. Wayve: Learning to drive in a day – 2018: Reinforcement learning (RL) is used to learn to drive from scratch by adapting a popular model-free deep reinforcement learning algorithm (deep deterministic policy gradients - DDPG) to solve the lane following task. The model input was a single monocular camera image. The system iterated through 3 processes: exploration, optimisation and evaluation. ","The network architecture was a deep network with 4 convolutional layers and 3 fully connected layers with a total of just under 10k parameters. For comparison, state of the art image classification architectures have 10s of millions of parameters. At every episode, a curved lane to follow is randomly generated, as well as the road texture and lane markings. The agent explores until it leaves the lane when the episode terminates. Then the policy optimises based on collected data and we repeat. ","One of the findings was that training the convolutional layers using an auto-encoder reconstruction loss significantly improved stability and data-efficiency of training. Learning to Drive from Simulation without Real World Labels - 2018: A model which learned to drive in simulation was transferred to the real world. The framework developed combines image translation and behavioural cloning. Instead of treating simulation and the real world as two distinct domains, a framework was designed for combining both where it is possible to train a steering policy in simulation while also exhibiting similar behaviour in the real world without ever seeing a real demonstration. For a detailed view of how we bridge both domains, our model is composed of a pair of convolutional variational auto-encoder style networks originally used for image translation (Unsupervised Image-to-Image Translation Networks, UNIT). This model translates images from one domain to another by first encoding the image, X, into a common latent space, Z, before decoding into the another domain. This part of the model is optimised to match the appearance of their respective domains with domain specific discriminators similar to cycle GANs along with a cycle consistent reconstruction losses. ","Learning to drive like a Human – 2019: Using just a sat-nav and a camera sensor, navigation was done around never-seen-before complex urban environments. It learns end-to-end with imitation learning (IL) and reinforcement learning (RL) to drive like a human, using computer vision to follow a route. Imitation learning allows copying of behaviours of expert human drivers. Reinforcement learning enables learning from each safety driver intervention to improve driving policy. The model learns both lateral and longitudinal control (steering and acceleration) of the vehicle with end-to-end deep learning. We propagate uncertainty throughout the model. This allows the learning of features from the input data which are most relevant for control, making computation very efficient. In fact, everything operates on the equivalent of a modern laptop computer. This massively reduces the sensor & compute cost (and power requirements) to less than 10% of traditional approaches. Learned Urban Driving - 2019: An extension to this imitation learning (IL) is conditional IL, which trains a neural network to perform different actions given some input in addition to the observed state. This is used to direct the car to follow a user-defined route. In short, this means collecting human driving data and training a machine learning model to copy this behaviour. Specifically, a simple motion plan is directly learnt from RGB images, and the plan is executed with the car. ","Wayve was inspired by a number of previous pieces of work: ▪ Nvidia’s demonstration of steering-only control and interpretability with end-to-end deep learning on novel roads without traffic. ▪ Demonstration of full vehicle control with conditional imitation learning in simulation and constrained environments with a toy RC car. ▪ Demonstration of learning steering-only control and localisation prediction with a richer learned route embedding. ▪ Learned approaches to parts of typical autonomous vehicle software stacks such as motion planning. … and more. Furthermore, Wayve highlighted the use of multiple learning signals for data efficiency: • Imitation of human expert drivers (supervised learning - IL), • Safety driver intervention data (negative reinforcement learning - RL) and corrective action (supervised learning), • Geometry, dynamics, motion and future prediction (self-supervised learning), • Labelled semantic computer vision data (supervised learning), • Simulation (supervised learning). End to end deep learning – 2021: Wayve’s system learns to drive with end-to-end deep learning, from sensory input to motion plan output. Hierarchical layers of abstraction are formed, and gradient signals are utilised throughout to optimise the entire neural network. The input to the system is a video stream from 6 monocular cameras and some supporting sensory and ordinary sat nav information. The neural network contains tens of millions of parameters and learns to regress a motion plan, which a controller is able to actuate on the vehicle. ","The system learns from a number of different signals and data sources with multi-task training. • Imitation learning from expert policy data • Reinforcement learning from safety driver intervention during on-policy testing • Safety driver corrective action following an intervention • Modelling dynamics and future state prediction from off-policy data • Intermediate computer vision representations of semantics, motion and geometry using supervised and self-supervised learning Combining these learning objectives and learning from large petabytes of driving data is what Wayve refers to as Fleet Learning. What’s particularly exciting is that Wayve did not need to hand-craft individual rules and policies to implement each of these behaviours, such as navigating road works, reacting to traffic lights or avoiding double-parked-vehicles. Rather, the end-to-end learning approach has learnt to generalise these scenarios from training data. ","Waymo: Waymo: In 2018, Waymo worked on an Imitation Learning (IL) technique -> ChauffeurNet - Learning to drive by imitating. Although IL performed well in real-world driving, it was not found to be competitive with existing motion planning techniques. Augmenting IL with exploration of rare and difficult scenarios via RL, was believed to be the key in improving the performance particularly for highly interactive scenarios. For Behaviour Prediction (BP) & Planning, this is Waymo’s high level architecture as of 2021: Techniques used by Waymo include a planning-based approach (iLQGame) and learning-based approach Scene Transformer (ST) to jointly plan for AV and predict for other agents. ","Approach 1 – Planning-based – iLQGame: Trajectory optimisation - Find a trajectory by optimising the cost function. The key part of the costs is parameterised by behaviour predictions of other agents. This is where the loop from the behaviour prediction (BP) to the planner exists. ","Predicting the behaviour of other agents can be thought of as also a planning problem, it’s just planning from the perspectives of other agents instead of the AV. Think of it as having separate cost function for each of the other agents and running iLQR independently on them like the AV. The solution is to formulate the problem into a linear quadratic (LQR) game, instead of trajectory optimisation, and solve a joint optimisation. Approach 2 – Learning-based – Scene Transformer: Scene Transformer - Learning-based approach for jointly predicting and planning. This is a technique that is commonly used for NLP, which was adapted for the self-driving vehicle problem of BP + Planning. ","The architecture of the transformer-based model, showing the inputs, outputs, and modules. ","","Aurora: State of the art Graph Neural Network (GNN) -> Geometric Transformer is employed to jointly reason over the graph of interactions between actor nodes trained from expert human driving data. (Supervised Learning | Imitation Learning). The graph structure naturally generalises different scenarios. Each actor is a node in the graph, edges in this graph, contain relationships (e.g. is actorX yielding to actorY?) Then it’s narrowed down to a subset of actors that directly affect the AV’s behaviour, allowing only the relevant interactions to be considered, and the rest is abstracted. For forecasting (prediction), learned attention model is used to generate a matrix of probabilities, for every actor, on every lane. "]